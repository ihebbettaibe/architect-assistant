{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import logging\n",
    "\n",
    "class TunisiaRealEstateScraper:\n",
    "    def __init__(self, base_url: str, delay: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the scraper\n",
    "        \n",
    "        Args:\n",
    "            base_url: Base URL of the real estate website\n",
    "            delay: Delay between requests in seconds (be respectful)\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Set headers to mimic a real browser\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        Fetch and parse a webpage\n",
    "        \n",
    "        Args:\n",
    "            url: URL to fetch\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup object or None if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Fetching: {url}\")\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Add delay to be respectful\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            self.logger.error(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_price(self, price_text: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Extract numeric price from text\n",
    "        \n",
    "        Args:\n",
    "            price_text: Text containing price\n",
    "            \n",
    "        Returns:\n",
    "            Numeric price or None\n",
    "        \"\"\"\n",
    "        if not price_text:\n",
    "            return None\n",
    "            \n",
    "        # Remove common currency symbols and text\n",
    "        price_text = re.sub(r'[^\\d.,]', '', price_text.replace(',', ''))\n",
    "        \n",
    "        try:\n",
    "            return float(price_text)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    def extract_area(self, area_text: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Extract area in square meters\n",
    "        \n",
    "        Args:\n",
    "            area_text: Text containing area\n",
    "            \n",
    "        Returns:\n",
    "            Area in square meters or None\n",
    "        \"\"\"\n",
    "        if not area_text:\n",
    "            return None\n",
    "            \n",
    "        # Look for patterns like \"120 m²\", \"120m2\", \"120 sq m\"\n",
    "        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:m²|m2|sq\\s*m)', area_text.lower())\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def scrape_generic_listings(self, \n",
    "                              listing_page_url: str,\n",
    "                              listing_selector: str,\n",
    "                              selectors: Dict[str, str],\n",
    "                              max_pages: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generic method to scrape real estate listings\n",
    "        \n",
    "        Args:\n",
    "            listing_page_url: URL of the listings page\n",
    "            listing_selector: CSS selector for individual listings\n",
    "            selectors: Dict mapping data fields to CSS selectors\n",
    "            max_pages: Maximum pages to scrape\n",
    "            \n",
    "        Returns:\n",
    "            List of property dictionaries\n",
    "        \"\"\"\n",
    "        properties = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # Construct page URL (common patterns)\n",
    "            if '?' in listing_page_url:\n",
    "                page_url = f\"{listing_page_url}&page={page}\"\n",
    "            else:\n",
    "                page_url = f\"{listing_page_url}?page={page}\"\n",
    "            \n",
    "            soup = self.get_page(page_url)\n",
    "            if not soup:\n",
    "                continue\n",
    "            \n",
    "            listings = soup.select(listing_selector)\n",
    "            if not listings:\n",
    "                self.logger.info(f\"No listings found on page {page}\")\n",
    "                break\n",
    "            \n",
    "            self.logger.info(f\"Found {len(listings)} listings on page {page}\")\n",
    "            \n",
    "            for listing in listings:\n",
    "                property_data = self.extract_listing_data(listing, selectors)\n",
    "                if property_data:\n",
    "                    properties.append(property_data)\n",
    "        \n",
    "        return properties\n",
    "    \n",
    "    def extract_listing_data(self, listing, selectors: Dict[str, str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data from a single listing\n",
    "        \n",
    "        Args:\n",
    "            listing: BeautifulSoup element for the listing\n",
    "            selectors: Dict mapping data fields to CSS selectors\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with extracted data\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        \n",
    "        for field, selector in selectors.items():\n",
    "            try:\n",
    "                element = listing.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    \n",
    "                    # Process specific fields\n",
    "                    if field == 'price':\n",
    "                        data[field] = self.extract_price(text)\n",
    "                        data['price_text'] = text\n",
    "                    elif field == 'area':\n",
    "                        data[field] = self.extract_area(text)\n",
    "                        data['area_text'] = text\n",
    "                    elif field == 'link':\n",
    "                        href = element.get('href')\n",
    "                        if href:\n",
    "                            data[field] = urljoin(self.base_url, href)\n",
    "                    else:\n",
    "                        data[field] = text\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error extracting {field}: {e}\")\n",
    "                data[field] = None\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def scrape_mubawab_tn(self, max_pages: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scraper for Mubawab.tn - Leading Tunisian real estate site\n",
    "        \"\"\"\n",
    "        selectors = {\n",
    "            'title': '.listingBox_title, .listingTitleLink, h3 a',\n",
    "            'price': '.listingBox_price, .priceTag, .price',\n",
    "            'location': '.listingBox_location, .address, .location',\n",
    "            'area': '.listingBox_surface, .surface, .area',\n",
    "            'rooms': '.listingBox_rooms, .rooms, .bed',\n",
    "            'type': '.listingBox_type, .propertyType',\n",
    "            'link': '.listingTitleLink, .listingBox_title a, h3 a',\n",
    "            'image': '.listingBox_photo img, .propertyPhoto img'\n",
    "        }\n",
    "        \n",
    "        return self.scrape_generic_listings(\n",
    "            listing_page_url=\"https://www.mubawab.tn/fr/cc/immobilier-vente\",\n",
    "            listing_selector=\".listingBox, .propertyCard, .listing-item\",\n",
    "            selectors=selectors,\n",
    "            max_pages=max_pages\n",
    "        )\n",
    "    \n",
    "    def scrape_tayara_tn(self, max_pages: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scraper for Tayara.tn - Popular Tunisian classified ads site\n",
    "        \"\"\"\n",
    "        selectors = {\n",
    "            'title': '.card-title a, .ad-title, .listing-title',\n",
    "            'price': '.card-price, .price, .ad-price',\n",
    "            'location': '.card-location, .location, .ad-location',\n",
    "            'area': '.card-details, .details, .surface',\n",
    "            'link': '.card-title a, .ad-title a',\n",
    "            'image': '.card-image img, .ad-image img'\n",
    "        }\n",
    "        \n",
    "        return self.scrape_generic_listings(\n",
    "            listing_page_url=\"https://www.tayara.tn/c/Immobilier\",\n",
    "            listing_selector=\".card, .ad-item, .listing-card\",\n",
    "            selectors=selectors,\n",
    "            max_pages=max_pages\n",
    "        )\n",
    "    \n",
    "    def scrape_tecnocasa_tn(self, max_pages: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scraper for Tecnocasa.tn - Italian real estate agency in Tunisia\n",
    "        \"\"\"\n",
    "        selectors = {\n",
    "            'title': '.property-title, .title, h3',\n",
    "            'price': '.property-price, .price, .cost',\n",
    "            'location': '.property-location, .location, .address',\n",
    "            'area': '.property-surface, .surface, .area',\n",
    "            'rooms': '.property-rooms, .rooms, .bed',\n",
    "            'type': '.property-type, .type',\n",
    "            'link': '.property-link, .title a, h3 a'\n",
    "        }\n",
    "        \n",
    "        return self.scrape_generic_listings(\n",
    "            listing_page_url=\"https://www.tecnocasa.tn/immobilier-a-vendre\",\n",
    "            listing_selector=\".property-item, .property-card, .listing\",\n",
    "            selectors=selectors,\n",
    "            max_pages=max_pages\n",
    "        )\n",
    "    \n",
    "    def save_to_csv(self, properties: List[Dict], filename: str):\n",
    "        \"\"\"\n",
    "        Save scraped data to CSV\n",
    "        \n",
    "        Args:\n",
    "            properties: List of property dictionaries\n",
    "            filename: Output CSV filename\n",
    "        \"\"\"\n",
    "        if not properties:\n",
    "            self.logger.warning(\"No properties to save\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(properties)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        self.logger.info(f\"Saved {len(properties)} properties to {filename}\")\n",
    "    \n",
    "    def save_to_json(self, properties: List[Dict], filename: str):\n",
    "        \"\"\"\n",
    "        Save scraped data to JSON\n",
    "        \n",
    "        Args:\n",
    "            properties: List of property dictionaries\n",
    "            filename: Output JSON filename\n",
    "        \"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(properties, f, ensure_ascii=False, indent=2)\n",
    "        self.logger.info(f\"Saved {len(properties)} properties to {filename}\")\n",
    "\n",
    "# Example usage and custom scraper functions\n",
    "def scrape_custom_site(base_url: str, custom_selectors: Dict[str, str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Function to scrape a custom real estate site\n",
    "    \n",
    "    Args:\n",
    "        base_url: Base URL of the site\n",
    "        custom_selectors: Dictionary mapping fields to CSS selectors\n",
    "        \n",
    "    Returns:\n",
    "        List of scraped properties\n",
    "    \"\"\"\n",
    "    scraper = TunisiaRealEstateScraper(base_url)\n",
    "    \n",
    "    # You need to analyze the website structure and update these\n",
    "    return scraper.scrape_generic_listings(\n",
    "        listing_page_url=f\"{base_url}/listings\",  # Adjust this\n",
    "        listing_selector=\".property-card\",        # Adjust this\n",
    "        selectors=custom_selectors,\n",
    "        max_pages=5\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate scraping real Tunisian real estate websites\n",
    "    \"\"\"\n",
    "    print(\"Tunisia Real Estate Scraper\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example 1: Scrape Mubawab.tn (Leading Tunisian real estate site)\n",
    "    print(\"\\n1. Scraping Mubawab.tn...\")\n",
    "    mubawab_scraper = TunisiaRealEstateScraper(\"https://www.mubawab.tn\")\n",
    "    mubawab_properties = mubawab_scraper.scrape_mubawab_tn(max_pages=2)\n",
    "    \n",
    "    if mubawab_properties:\n",
    "        mubawab_scraper.save_to_csv(mubawab_properties, \"mubawab_properties.csv\")\n",
    "        print(f\"✓ Scraped {len(mubawab_properties)} properties from Mubawab.tn\")\n",
    "    \n",
    "    # Example 2: Scrape Tayara.tn\n",
    "    print(\"\\n2. Scraping Tayara.tn...\")\n",
    "    tayara_scraper = TunisiaRealEstateScraper(\"https://www.tayara.tn\")\n",
    "    tayara_properties = tayara_scraper.scrape_tayara_tn(max_pages=2)\n",
    "    \n",
    "    if tayara_properties:\n",
    "        tayara_scraper.save_to_csv(tayara_properties, \"tayara_properties.csv\")\n",
    "        print(f\"✓ Scraped {len(tayara_properties)} properties from Tayara.tn\")\n",
    "    \n",
    "    # Example 3: Scrape Tecnocasa.tn\n",
    "    print(\"\\n3. Scraping Tecnocasa.tn...\")\n",
    "    tecnocasa_scraper = TunisiaRealEstateScraper(\"https://www.tecnocasa.tn\")\n",
    "    tecnocasa_properties = tecnocasa_scraper.scrape_tecnocasa_tn(max_pages=2)\n",
    "    \n",
    "    if tecnocasa_properties:\n",
    "        tecnocasa_scraper.save_to_csv(tecnocasa_properties, \"tecnocasa_properties.csv\")\n",
    "        print(f\"✓ Scraped {len(tecnocasa_properties)} properties from Tecnocasa.tn\")\n",
    "    \n",
    "    # Combine all properties\n",
    "    all_properties = []\n",
    "    if mubawab_properties:\n",
    "        for prop in mubawab_properties:\n",
    "            prop['source'] = 'Mubawab.tn'\n",
    "        all_properties.extend(mubawab_properties)\n",
    "    \n",
    "    if tayara_properties:\n",
    "        for prop in tayara_properties:\n",
    "            prop['source'] = 'Tayara.tn'\n",
    "        all_properties.extend(tayara_properties)\n",
    "    \n",
    "    if tecnocasa_properties:\n",
    "        for prop in tecnocasa_properties:\n",
    "            prop['source'] = 'Tecnocasa.tn'\n",
    "        all_properties.extend(tecnocasa_properties)\n",
    "    \n",
    "    # Save combined results\n",
    "    if all_properties:\n",
    "        combined_scraper = TunisiaRealEstateScraper(\"https://combined.tn\")\n",
    "        combined_scraper.save_to_csv(all_properties, \"tunisia_all_properties.csv\")\n",
    "        combined_scraper.save_to_json(all_properties, \"tunisia_all_properties.json\")\n",
    "        \n",
    "        print(f\"\\n🎉 Total scraped: {len(all_properties)} properties from all sites\")\n",
    "        print(\"📁 Files saved:\")\n",
    "        print(\"   - mubawab_properties.csv\")\n",
    "        print(\"   - tayara_properties.csv\") \n",
    "        print(\"   - tecnocasa_properties.csv\")\n",
    "        print(\"   - tunisia_all_properties.csv (combined)\")\n",
    "        print(\"   - tunisia_all_properties.json (combined)\")\n",
    "        \n",
    "        # Display sample results\n",
    "        if all_properties:\n",
    "            print(\"\\n📋 Sample property from results:\")\n",
    "            sample_prop = all_properties[0]\n",
    "            for key, value in sample_prop.items():\n",
    "                if value and key not in ['link', 'image']:  # Skip long URLs\n",
    "                    print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"🚀 USAGE INSTRUCTIONS:\")\n",
    "    print(\"1. Install: pip install requests beautifulsoup4 pandas\")\n",
    "    print(\"2. Run this script to scrape real Tunisian real estate sites\")\n",
    "    print(\"3. Check output CSV/JSON files for scraped data\")\n",
    "    print(\"4. Modify selectors if website structure changes\")\n",
    "    print(\"\\n⚠️  IMPORTANT NOTES:\")\n",
    "    print(\"• Always check robots.txt before scraping\")\n",
    "    print(\"• Respect rate limits (current: 1 second delay)\")\n",
    "    print(\"• Some sites may block automated requests\")\n",
    "    print(\"• CSS selectors may change - update as needed\")\n",
    "    print(\"• Consider using proxies for large-scale scraping\")\n",
    "\n",
    "# Additional utility functions for specific Tunisian real estate sites\n",
    "def scrape_multiple_tunisia_sites(max_pages_per_site: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape multiple Tunisian real estate sites and return combined DataFrame\n",
    "    \n",
    "    Args:\n",
    "        max_pages_per_site: Maximum pages to scrape per site\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with all properties\n",
    "    \"\"\"\n",
    "    all_properties = []\n",
    "    \n",
    "    # Site configurations\n",
    "    sites = [\n",
    "        {\n",
    "            'name': 'Mubawab.tn',\n",
    "            'scraper_method': 'scrape_mubawab_tn',\n",
    "            'base_url': 'https://www.mubawab.tn'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Tayara.tn', \n",
    "            'scraper_method': 'scrape_tayara_tn',\n",
    "            'base_url': 'https://www.tayara.tn'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Tecnocasa.tn',\n",
    "            'scraper_method': 'scrape_tecnocasa_tn', \n",
    "            'base_url': 'https://www.tecnocasa.tn'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for site in sites:\n",
    "        try:\n",
    "            print(f\"Scraping {site['name']}...\")\n",
    "            scraper = TunisiaRealEstateScraper(site['base_url'])\n",
    "            method = getattr(scraper, site['scraper_method'])\n",
    "            properties = method(max_pages=max_pages_per_site)\n",
    "            \n",
    "            # Add source information\n",
    "            for prop in properties:\n",
    "                prop['source'] = site['name']\n",
    "                prop['scraped_at'] = pd.Timestamp.now()\n",
    "            \n",
    "            all_properties.extend(properties)\n",
    "            print(f\"✓ Got {len(properties)} properties from {site['name']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error scraping {site['name']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(all_properties) if all_properties else pd.DataFrame()\n",
    "\n",
    "def analyze_tunisia_property_prices(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Basic analysis of scraped Tunisian property data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with property data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {\"error\": \"No data provided\"}\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Price analysis\n",
    "    if 'price' in df.columns:\n",
    "        prices = df['price'].dropna()\n",
    "        if not prices.empty:\n",
    "            analysis['price_stats'] = {\n",
    "                'avg_price': prices.mean(),\n",
    "                'median_price': prices.median(),\n",
    "                'min_price': prices.min(),\n",
    "                'max_price': prices.max(),\n",
    "                'total_properties': len(prices)\n",
    "            }\n",
    "    \n",
    "    # Location analysis\n",
    "    if 'location' in df.columns:\n",
    "        locations = df['location'].value_counts().head(10)\n",
    "        analysis['top_locations'] = locations.to_dict()\n",
    "    \n",
    "    # Source analysis\n",
    "    if 'source' in df.columns:\n",
    "        sources = df['source'].value_counts()\n",
    "        analysis['properties_per_source'] = sources.to_dict()\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
